{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This repository includes a set of documents for best practices around data replication between two Kafka clusters. Mirror Maker 2.0 Mirror Maker 2.0 is the new replication feature of Kafka 2.4. It was defined as part of the KIP 382 . General concepts As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note . The figure below illustrates the mirror maker internal components running within Kafka Connect. In distributed mode, Mirror Maker creates the following topics to the target cluster: mm2-configs.source.internal: This topic will store the connector and task configurations. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic will store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical mirror maker configuration is done via property file and defines source and target clusters with their connection properties and the replication flow definitions. Here is a simple example for a local cluster to a target cluster using TLS v1.2 for connection encryption and Sasl authentication protocol to connect to Event Streams. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] but can be also specified with the properties: topics.blacklist . Comma-separated lists are also supported and Java regular expression. Internally MirrorSourceConnector and MirrorCheckpointConnector will create multiple tasks (up to tasks.max property), MirrorHeartbeatConnector creates only one single task. MirrorSourceConnector will have one task per topic-partition to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka connect framework uses the coordinator API, with the assign() API, so there is no consumer group while fetching data from source topic. There is no call to commit() neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern. Requirements Local cluster to Event Streams on Cloud The goal is to demonstrate the replicate data from local cluster to Event Streams on IBM Cloud running as managed service. The two scenarios and the step by step approach are presented in this note . We have also documented the replication approaches from Event Streams as a Service to local cluster in this note . Provisioning Connectors (Mirror Maker 2) This main epic is related to provisioning operation. As a SRE I want to provision and deploy Mirror Maker 2 connector to existing Openshift cluster without exposing password and keys so replication can start working. This will use Kubernetes secrets for configuration parameters. As a SRE I want to understand the CLI commands used to assess how automation can be performed for replicating environment provisioning. As a SRE I want to understand the server sizing for the Mirror Maker environment. Note that, there is no specific user interface for mirror maker connector. Version to version migration As a SRE, I want to understand how to perform a version to version migration for the Mirror Maker 2 product so that existing streaming replication is not impacted by the upgrade. As a developer I want to deploy configuration updates to modify the topic white or black lists so that newly added topics are replicated. Security As a SRE, I want to understand how the security support to connect client applications to cluster and to replicated topic. As a developer I want to design Mirror Maker 2 based replication solution to support different line of businesses who should not connect to topics and data not related to their business and security scope. Monitoring As a SRE, I want to get Mirror Maker 2 metrics for Prometheus so that it fits in my current metrics processing practices. The explanation to setup Prometheus metris for mirror maker 2.0 is documented here . As a SRE, I want to be able to add new dashboard into Grafana to visualize the Mirror Maker 2 metrics. As a SRE, I want to define rules for alert reporting and configure a Slack channel for alerting. As a SRE, I want to get the Mirror Maker 2 logs into our Splunk logging platform. Best Practices As a developer I want to understand how Mirror Maker 2 based replication address the record duplication. As a developer I want to design replication solution to minimize the instance of Mirror Maker or being able to scale them if I observe lag into data replication processing. As a developer I want to understand what are the condition for message loss. Performance tests As a developer I want to understand how to measure data latency and lag in data replication. As a SRE I want to understand current thoughput for the replication solution.","title":"Introduction"},{"location":"#introduction","text":"This repository includes a set of documents for best practices around data replication between two Kafka clusters.","title":"Introduction"},{"location":"#mirror-maker-20","text":"Mirror Maker 2.0 is the new replication feature of Kafka 2.4. It was defined as part of the KIP 382 .","title":"Mirror Maker 2.0"},{"location":"#general-concepts","text":"As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note . The figure below illustrates the mirror maker internal components running within Kafka Connect. In distributed mode, Mirror Maker creates the following topics to the target cluster: mm2-configs.source.internal: This topic will store the connector and task configurations. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic will store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical mirror maker configuration is done via property file and defines source and target clusters with their connection properties and the replication flow definitions. Here is a simple example for a local cluster to a target cluster using TLS v1.2 for connection encryption and Sasl authentication protocol to connect to Event Streams. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] but can be also specified with the properties: topics.blacklist . Comma-separated lists are also supported and Java regular expression. Internally MirrorSourceConnector and MirrorCheckpointConnector will create multiple tasks (up to tasks.max property), MirrorHeartbeatConnector creates only one single task. MirrorSourceConnector will have one task per topic-partition to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka connect framework uses the coordinator API, with the assign() API, so there is no consumer group while fetching data from source topic. There is no call to commit() neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern.","title":"General concepts"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#local-cluster-to-event-streams-on-cloud","text":"The goal is to demonstrate the replicate data from local cluster to Event Streams on IBM Cloud running as managed service. The two scenarios and the step by step approach are presented in this note . We have also documented the replication approaches from Event Streams as a Service to local cluster in this note .","title":"Local cluster to Event Streams on Cloud"},{"location":"#provisioning-connectors-mirror-maker-2","text":"This main epic is related to provisioning operation. As a SRE I want to provision and deploy Mirror Maker 2 connector to existing Openshift cluster without exposing password and keys so replication can start working. This will use Kubernetes secrets for configuration parameters. As a SRE I want to understand the CLI commands used to assess how automation can be performed for replicating environment provisioning. As a SRE I want to understand the server sizing for the Mirror Maker environment. Note that, there is no specific user interface for mirror maker connector.","title":"Provisioning Connectors (Mirror Maker 2)"},{"location":"#version-to-version-migration","text":"As a SRE, I want to understand how to perform a version to version migration for the Mirror Maker 2 product so that existing streaming replication is not impacted by the upgrade. As a developer I want to deploy configuration updates to modify the topic white or black lists so that newly added topics are replicated.","title":"Version to version migration"},{"location":"#security","text":"As a SRE, I want to understand how the security support to connect client applications to cluster and to replicated topic. As a developer I want to design Mirror Maker 2 based replication solution to support different line of businesses who should not connect to topics and data not related to their business and security scope.","title":"Security"},{"location":"#monitoring","text":"As a SRE, I want to get Mirror Maker 2 metrics for Prometheus so that it fits in my current metrics processing practices. The explanation to setup Prometheus metris for mirror maker 2.0 is documented here . As a SRE, I want to be able to add new dashboard into Grafana to visualize the Mirror Maker 2 metrics. As a SRE, I want to define rules for alert reporting and configure a Slack channel for alerting. As a SRE, I want to get the Mirror Maker 2 logs into our Splunk logging platform.","title":"Monitoring"},{"location":"#best-practices","text":"As a developer I want to understand how Mirror Maker 2 based replication address the record duplication. As a developer I want to design replication solution to minimize the instance of Mirror Maker or being able to scale them if I observe lag into data replication processing. As a developer I want to understand what are the condition for message loss.","title":"Best Practices"},{"location":"#performance-tests","text":"As a developer I want to understand how to measure data latency and lag in data replication. As a SRE I want to understand current thoughput for the replication solution.","title":"Performance  tests"},{"location":"es-prem-to-es/","text":"","title":"Es prem to es"},{"location":"es-to-local/","text":"To Event Streams to Kafka cluster on-premise Scenario 3: From Event Streams to local cluster For this scenario the source is Event Streams on IBM Cloud and the target is a local server (may be on a laptop using vanilla Kafka image (Strimzi kafka 2.4 docker image) started with docker compose). This target cluster runs two zookeeper nodes, and three kafka nodes. We need 3 kafka brokers as mirror maker created topics with a replication factor set to 3. This time the producer adds headers to the Records sent so we can validate headers replication. The file es-cluster/es-mirror-maker.properties declares the mirroring settings as below: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders Start the target cluster runnning on your laptop using: docker-compose up Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate: clusters = source, target source.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 source.security.protocol = SSL source.ssl.truststore.password = password source.ssl.truststore.location = /home/truststore.jks target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders As the source cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by those Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. When running from a remote system to get the certificate do the following steps: Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt Transform the certificate fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt For Openshift or Kubernetes deployment, the mirror maker descriptor needs to declare the TLS stamza: mirrors : - sourceCluster : \"my-cluster-source\" targetCluster : \"my-cluster-target\" sourceConnector : config : replication.factor : 1 offset-syncs.topic.replication.factor : 1 sync.topic.acls.enabled : \"false\" targetConnector : tls : trustedCertificates : - secretName : my-cluster-cluster-cert certificate : ca.crt The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster. Produce some records to products topic on Event Streams: export KAFKA_BROKERS = \"event streams broker list\" export KAFKA_APIKEY = \"event stream api key\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_APIKEY = $KAFKA_APIKEY -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" To validate the target source.products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic source.products --from-beginning","title":"Replication from Event Streams to local"},{"location":"es-to-local/#to-event-streams-to-kafka-cluster-on-premise","text":"","title":"To Event Streams to Kafka cluster on-premise"},{"location":"es-to-local/#scenario-3-from-event-streams-to-local-cluster","text":"For this scenario the source is Event Streams on IBM Cloud and the target is a local server (may be on a laptop using vanilla Kafka image (Strimzi kafka 2.4 docker image) started with docker compose). This target cluster runs two zookeeper nodes, and three kafka nodes. We need 3 kafka brokers as mirror maker created topics with a replication factor set to 3. This time the producer adds headers to the Records sent so we can validate headers replication. The file es-cluster/es-mirror-maker.properties declares the mirroring settings as below: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders Start the target cluster runnning on your laptop using: docker-compose up Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate: clusters = source, target source.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 source.security.protocol = SSL source.ssl.truststore.password = password source.ssl.truststore.location = /home/truststore.jks target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders As the source cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by those Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. When running from a remote system to get the certificate do the following steps: Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt Transform the certificate fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt For Openshift or Kubernetes deployment, the mirror maker descriptor needs to declare the TLS stamza: mirrors : - sourceCluster : \"my-cluster-source\" targetCluster : \"my-cluster-target\" sourceConnector : config : replication.factor : 1 offset-syncs.topic.replication.factor : 1 sync.topic.acls.enabled : \"false\" targetConnector : tls : trustedCertificates : - secretName : my-cluster-cluster-cert certificate : ca.crt The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders","title":"Scenario 3: From Event Streams to local cluster"},{"location":"es-to-local/#scenario-4-from-event-streams-on-cloud-to-strimzi-cluster-on-openshift","text":"We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster. Produce some records to products topic on Event Streams: export KAFKA_BROKERS = \"event streams broker list\" export KAFKA_APIKEY = \"event stream api key\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_APIKEY = $KAFKA_APIKEY -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" To validate the target source.products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic source.products --from-beginning","title":"Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift"},{"location":"local-to-es/","text":"From Local cluster to Event Streams We propose two approaches to run the 'local' cluster: Docker compose vanilla Kafka 2.4 : To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in mirror-maker-2/local-cluster folder. This compose file uses a local docker network called kafkanet . The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version. Openshift deployed Kafka 2.4 cluster using Strimzi operator Pre-requisites You need to have one Event Streams service created on IBM Cloud. You may need to use Event Streams CLI. So follow those instructions to get it. The following ibmcloud CLI command presents the Event Stream cluster's metadata, like the broker list and the cluster ID: ibmcloud es cluster For other CLI commands see this summary . Scenario 1: From Kafka local as source to Event Streams on Cloud as Target The test scenario goal is to send the product definitions in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY: export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. Normally we observed with Event streams APIKEY it is possible to create topic with AdminClient, so there is no need to do the following commands. For other configuration where Access Control policies do not authorize program to create topic dynamically, the commands performed by and admin user will create the needed topic. (the mm2 prefix is the one used by mirror maker, but the name of the topic could be defined in the mirror maker properties) ibmcloud es topic-create -n mm2-configs.source.internal -p 1 -c cleanup.policy = compact ibmcloud es topic-create -n mm2-offsets.source.internal -p 25 -c cleanup.policy = compact ibmcloud es topic-create -n mm2-status.source.internal -p 5 -c cleanup.policy = compact ibmcloud es topic-create -n source.products -p 1 ibmcloud es topic-create -n source.heartbeats -p 1 -c cleanup.policy = compact ibmcloud es topic-create -n source.checkpoints.internal -p 1 -c cleanup.policy = compact In one Terminal window, start the local cluster using docker-compose under the mirror-maker-2/local-cluster folder: docker-compose up & . The data are persisted on the local disk in this folder. If this is the first time you started the source cluster, you need to create the products topic. Start a Kafka container to access the Kafka tools with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify topic is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command to build this image (if you change the image name be sure to use the new name in future command) is: docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In this isolated python container bash shell do the following to send the 5 first products: $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams cluster properties file for the different Kafka tool commands. Set the password attribute of the jaas.config to match Event Streams APIKEY. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to Event Streams and with the replicated topic: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0, close to the local cluster, using, yet another docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This launchMM2.sh script is updating a template properties file with the values of the environment variables and calls with this updated file: /opt/kafka/bin/connect-mirror-maker.sh mm2.properties The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder within the docker container. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving to the source.topics after the replication complete. { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Scenario 2: Run Mirror Maker 2 Cluster close to target cluster This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster: We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster. Producers are running locally on the same OpenShift cluster, where vanilla Kafka 2.4 is running, or can run remotely using exposed Kafka brokers Openshift route. (The black rectangles in the figure above represent those producers.) The goal is to replicate the products topic from the left to the source.products to the right. What needs to be done: Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction . If not done yet, create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-api-secret --from-literal=password=<replace-with-event-streams-apikey> Create a project in OpenShift to deploy Mirror Maker cluster, for example: oc new-project mirror-maker-2-to-es . At the minimum, to run Mirror Maker 2, we need to deploy the Strimzi Custom Resource Definitions, and the Mirror Maker 2.0 operator. See the detail in the section \"\" from the provisioning note . As the vanilla Kafka source cluster is using TLS to communicate between clients and brokers, we need to use the k8s secret defined when deploying Kafka which includes the CAroot certificate. This secret is : my-cluster-clients-ca-cert . # build a local CA crt file from the secret: oc extract secret/my-cluster-clients-ca-cert --keys = ca.crt --to = - > ca.crt # Verify the certificate: openssl x509 -in ca.crt -text # transform it for java truststore.jks: keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt # create a secret from file the truststore so it can be mounted as needed oc create secret generic kafka-truststore --from-file = ./truststore.jks # Verify the created secret oc describe secret kafka-truststore Attention At this step, we have two options to deploy mirror maker, one using the Mirror Maker Operator and configur it via a yaml file, or use properties file and a special docker image that is deployed to Openshift. As of 3/20/2020 we have found an issue on Strimzi 0.17-rc2 Mirror Maker 2.0 operator, so we are proposing to use the properties approach as documented in this separated note . Deploy Mirror Maker as a custom image In this approach, documented in a separate note , we are using properties file to define the Mirror Maker 2.0 configuration, package JMX exporter with it inside a docker image and deploy the image to Openshift. Deploy with Strimzi Mirror Maker 2.0 Operator Define source and target cluster properties in mirror maker 2.0 kafka-to-es-mm2.yml descriptor file. We strongly recommend to study the schema definition of this custom resource from this page . The yaml file we used is here . Note connectCluster defined the cluster alias used for Kafka Connect, it must match a cluster in the list at spec.clusters . The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. Also we have some challenges to make the connection to event streams working, as of Strimzi version 0.17 RC2, we need to add an empty tls: {} stanza to get connected. Also below, the declaration is using the previously defined secret for event streams API key. alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3... tls : {} authentication : passwordSecret : secretName : es-api-secret password : password username : token type : plain Deploy Mirror maker 2.0 within your project. oc apply -f kafka-to-es-mm2.yaml This commmand creates a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topics to replicate have multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers. Validate data replication To validate the replication works, we will connect a consumer to the source.products topic on Event Streams. So we define a target cluster property file ( eventstreams.properties ) like: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; Start the consumer on source.products topic running in Event Streams on the cloud: we use a setenv.sh shell to export the needed environment variables docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash bash-4.2$ source /home/setenv.sh bash-4.2$ ./bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: export KAFKA_BROKERS = \"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.crt\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" As an alternate solution you can run the producer as a pod inside of the source cluster then send the product one by one using the console: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products If you don t see a command prompt, try pressing enter. > { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } > { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } There is other solution, like using an Kafka HTTP brigde and use curl post to send the records To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning In the terminal where kafka consumer runs to get message from source.products you should see the data replicated.","title":"Replication from local to Event Streams"},{"location":"local-to-es/#from-local-cluster-to-event-streams","text":"We propose two approaches to run the 'local' cluster: Docker compose vanilla Kafka 2.4 : To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in mirror-maker-2/local-cluster folder. This compose file uses a local docker network called kafkanet . The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version. Openshift deployed Kafka 2.4 cluster using Strimzi operator","title":"From Local cluster to Event Streams"},{"location":"local-to-es/#pre-requisites","text":"You need to have one Event Streams service created on IBM Cloud. You may need to use Event Streams CLI. So follow those instructions to get it. The following ibmcloud CLI command presents the Event Stream cluster's metadata, like the broker list and the cluster ID: ibmcloud es cluster For other CLI commands see this summary .","title":"Pre-requisites"},{"location":"local-to-es/#scenario-1-from-kafka-local-as-source-to-event-streams-on-cloud-as-target","text":"The test scenario goal is to send the product definitions in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY: export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. Normally we observed with Event streams APIKEY it is possible to create topic with AdminClient, so there is no need to do the following commands. For other configuration where Access Control policies do not authorize program to create topic dynamically, the commands performed by and admin user will create the needed topic. (the mm2 prefix is the one used by mirror maker, but the name of the topic could be defined in the mirror maker properties) ibmcloud es topic-create -n mm2-configs.source.internal -p 1 -c cleanup.policy = compact ibmcloud es topic-create -n mm2-offsets.source.internal -p 25 -c cleanup.policy = compact ibmcloud es topic-create -n mm2-status.source.internal -p 5 -c cleanup.policy = compact ibmcloud es topic-create -n source.products -p 1 ibmcloud es topic-create -n source.heartbeats -p 1 -c cleanup.policy = compact ibmcloud es topic-create -n source.checkpoints.internal -p 1 -c cleanup.policy = compact In one Terminal window, start the local cluster using docker-compose under the mirror-maker-2/local-cluster folder: docker-compose up & . The data are persisted on the local disk in this folder. If this is the first time you started the source cluster, you need to create the products topic. Start a Kafka container to access the Kafka tools with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify topic is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command to build this image (if you change the image name be sure to use the new name in future command) is: docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In this isolated python container bash shell do the following to send the 5 first products: $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams cluster properties file for the different Kafka tool commands. Set the password attribute of the jaas.config to match Event Streams APIKEY. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to Event Streams and with the replicated topic: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0, close to the local cluster, using, yet another docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This launchMM2.sh script is updating a template properties file with the values of the environment variables and calls with this updated file: /opt/kafka/bin/connect-mirror-maker.sh mm2.properties The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder within the docker container. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving to the source.topics after the replication complete. { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 }","title":"Scenario 1: From Kafka local as source to Event Streams on Cloud as Target"},{"location":"local-to-es/#scenario-2-run-mirror-maker-2-cluster-close-to-target-cluster","text":"This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster: We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster. Producers are running locally on the same OpenShift cluster, where vanilla Kafka 2.4 is running, or can run remotely using exposed Kafka brokers Openshift route. (The black rectangles in the figure above represent those producers.) The goal is to replicate the products topic from the left to the source.products to the right. What needs to be done: Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction . If not done yet, create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-api-secret --from-literal=password=<replace-with-event-streams-apikey> Create a project in OpenShift to deploy Mirror Maker cluster, for example: oc new-project mirror-maker-2-to-es . At the minimum, to run Mirror Maker 2, we need to deploy the Strimzi Custom Resource Definitions, and the Mirror Maker 2.0 operator. See the detail in the section \"\" from the provisioning note . As the vanilla Kafka source cluster is using TLS to communicate between clients and brokers, we need to use the k8s secret defined when deploying Kafka which includes the CAroot certificate. This secret is : my-cluster-clients-ca-cert . # build a local CA crt file from the secret: oc extract secret/my-cluster-clients-ca-cert --keys = ca.crt --to = - > ca.crt # Verify the certificate: openssl x509 -in ca.crt -text # transform it for java truststore.jks: keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt # create a secret from file the truststore so it can be mounted as needed oc create secret generic kafka-truststore --from-file = ./truststore.jks # Verify the created secret oc describe secret kafka-truststore Attention At this step, we have two options to deploy mirror maker, one using the Mirror Maker Operator and configur it via a yaml file, or use properties file and a special docker image that is deployed to Openshift. As of 3/20/2020 we have found an issue on Strimzi 0.17-rc2 Mirror Maker 2.0 operator, so we are proposing to use the properties approach as documented in this separated note .","title":"Scenario 2: Run Mirror Maker 2 Cluster close to target cluster"},{"location":"local-to-es/#deploy-mirror-maker-as-a-custom-image","text":"In this approach, documented in a separate note , we are using properties file to define the Mirror Maker 2.0 configuration, package JMX exporter with it inside a docker image and deploy the image to Openshift.","title":"Deploy Mirror Maker as a custom image"},{"location":"local-to-es/#deploy-with-strimzi-mirror-maker-20-operator","text":"Define source and target cluster properties in mirror maker 2.0 kafka-to-es-mm2.yml descriptor file. We strongly recommend to study the schema definition of this custom resource from this page . The yaml file we used is here . Note connectCluster defined the cluster alias used for Kafka Connect, it must match a cluster in the list at spec.clusters . The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. Also we have some challenges to make the connection to event streams working, as of Strimzi version 0.17 RC2, we need to add an empty tls: {} stanza to get connected. Also below, the declaration is using the previously defined secret for event streams API key. alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3... tls : {} authentication : passwordSecret : secretName : es-api-secret password : password username : token type : plain Deploy Mirror maker 2.0 within your project. oc apply -f kafka-to-es-mm2.yaml This commmand creates a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topics to replicate have multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers.","title":"Deploy with Strimzi Mirror Maker 2.0 Operator"},{"location":"local-to-es/#validate-data-replication","text":"To validate the replication works, we will connect a consumer to the source.products topic on Event Streams. So we define a target cluster property file ( eventstreams.properties ) like: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; Start the consumer on source.products topic running in Event Streams on the cloud: we use a setenv.sh shell to export the needed environment variables docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash bash-4.2$ source /home/setenv.sh bash-4.2$ ./bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: export KAFKA_BROKERS = \"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.crt\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" As an alternate solution you can run the producer as a pod inside of the source cluster then send the product one by one using the console: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products If you don t see a command prompt, try pressing enter. > { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } > { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } There is other solution, like using an Kafka HTTP brigde and use curl post to send the records To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning In the terminal where kafka consumer runs to get message from source.products you should see the data replicated.","title":"Validate data replication"},{"location":"monitoring/","text":"Monitoring Mirror Maker and kafka connect cluster The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics to Prometheus and how to use Grafana dashboard. Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. The following figure presents the prometheus generic architecture as described from their main website. Basically the Prometheus server hosts job to poll HTTP end points to get metrics from the components to monitor. It supports queries in the format of PromQL , that product like Grafana can use to present nice dashboards, and it can push alerts to different channels when some metrics behave unexpectedly. In the context of data replication between kafka clusters, we want to monitor the mirror maker 2.0 metrics like the worker task states, source task metrics, task errors,... The following figure illustrates the components involved: The source Kafka cluster, the Mirror Maker 2.0 cluster, which is based on Kafka Connect, the Prometheus server and the Grafana. As all those components run on kubernetes, most of them could be deployed via Operators using Custom Resource Definitions. To support this monitoring we need to do the following steps: Add metrics configuration to your Mirror Maker 2.0 cluster Package the mirror maker 2 to use JMX Exporter as Java agent so it exposes JMX MBeans as metrics accessibles via HTTP. Deploy Prometheus via Opertors Optionally deploy Prometheus Alertmanager Deploy Grafana Installation and configuration Prometheus deployment inside Kubernetes uses operator as defined in the coreos github . The CRDs define a set of resources. The ServiceMonitor, PodMonitor, PrometheusRule are used. Inside the Strimzi github repository , we can get a prometheus.yml file to deploy prometheus server. This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance. We have defined our own configuration in this file . For your own deployment you have to change the target namespace, and the rules You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the Mirror Maker 2 Cluster. To be able to monitor your own on-premise Kafka cluster you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment with Prometheus setting can be found in Strimzi examples . The declarations are under the metrics stanza and define the rules to expose Kafka core features. Install Prometheus After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator by first downloading the different configuration yaml files and update the namespace declaration to reflect your project name (e.g jb-kafka-strimzi ): curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-deployment.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-operator-deployment.yaml curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role.yaml > prometheus-operator-cluster-role.yaml curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role-binding.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-operator-cluster-role-binding.yaml curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-service-account.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-operator-service-account.yaml Note The prometheus-operator-deployment.yaml defines security context the operator pod will use. It is set as a non root user (unprivileged). If you need to change that, or reference an existing user modify this file. Deploy the prometheus operator, cluster role, role binding and service account (see our files under monitoring folder): oc apply -f prometheus-operator-deployment.yaml oc apply -f prometheus-operator-cluster-role.yaml oc apply -f prometheus-operator-cluster-role-binding.yaml oc apply -f prometheus-operator-service-account.yaml When you apply those configurations, the following resources are managed by the Prometheus Operator: Resource Description ClusterRole To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To manage the configuration of the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource) Deploy prometheus Note The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker or Kafka Connect monitoring, the configuration will have less rules, and parameters. See next section . Deploy the prometheus server by first changing the namespace and also by adapting the original examples/metrics/prometheus-install/prometheus.yaml file . curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: jb-kafka-strimzi/\" > prometheus.yml If you are using AlertManager (see section below ) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-rules.yaml oc apply -f prometheus-rules.yaml oc apply -f prometheus.yaml The Prometheus server configuration uses service discovery to discover the pods (Mirror Maker 2.0 pod) in the cluster from which it gets metrics. Mirror maker 2.0 monitoring To monitor MM2 with Prometheus we need to add JMX Exporter and run it as Java agent.The jar file for JMX exporter agent can be found here . We copied a version in the folder mirror-maker-2/libs . We have adopted a custom mirror maker 2.0 docker imaged based on Kafka 2.4. We are detailing how to build this image using this Dockerfile in this separate note . We have used this approach as we have found an issue with the Strimzi Mirror Maker operator, that blocks us to continue the monitoring. We expect that htis operator, when it sees metrics declaration in the Mirror Maker 2 configuration yaml file, with use the JMX exporter jar. Once the Mirror Maker 2.0 is connected... Install Grafana Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker. Deploy Grafan to OpenShift and expose it via a service: oc apply -f grafana.yaml In case you want to test grafana locally run: docker run -d -p 3000:3000 grafana/grafana Kafka Explorer Configure Grafana dashboard To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service. Use port forwarding: export PODNAME = $( oc get pods -l name = grafana | grep grafana | awk '{print $1}' ) kubectl port-forward $PODNAME 3000 :3000 Point your browser to http://localhost:3000 . Expose the route via cli Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090 Alert Manager As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file The prometheus-rules.yaml . Those rules are used by the AlertManager component. Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager. Download an example of alert manager configuration file curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml > alert-manager.yaml Define a configuration for the channel to use, by starting from the following template curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml > alert-manager-config.yaml Modify this file to reflect the remote access credential and URL to the channel server. Then deploy the secret that matches your config file . oc create secret generic alertmanager-alertmanager --from-file = alertmanager.yaml = alert-manager-config.yaml oc create secret generic additional-scrape-configs --from-file = ./local-cluster/prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f -","title":"Monitoring with Prometheus and Grafana"},{"location":"monitoring/#monitoring-mirror-maker-and-kafka-connect-cluster","text":"The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics to Prometheus and how to use Grafana dashboard. Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. The following figure presents the prometheus generic architecture as described from their main website. Basically the Prometheus server hosts job to poll HTTP end points to get metrics from the components to monitor. It supports queries in the format of PromQL , that product like Grafana can use to present nice dashboards, and it can push alerts to different channels when some metrics behave unexpectedly. In the context of data replication between kafka clusters, we want to monitor the mirror maker 2.0 metrics like the worker task states, source task metrics, task errors,... The following figure illustrates the components involved: The source Kafka cluster, the Mirror Maker 2.0 cluster, which is based on Kafka Connect, the Prometheus server and the Grafana. As all those components run on kubernetes, most of them could be deployed via Operators using Custom Resource Definitions. To support this monitoring we need to do the following steps: Add metrics configuration to your Mirror Maker 2.0 cluster Package the mirror maker 2 to use JMX Exporter as Java agent so it exposes JMX MBeans as metrics accessibles via HTTP. Deploy Prometheus via Opertors Optionally deploy Prometheus Alertmanager Deploy Grafana","title":"Monitoring Mirror Maker and kafka connect cluster"},{"location":"monitoring/#installation-and-configuration","text":"Prometheus deployment inside Kubernetes uses operator as defined in the coreos github . The CRDs define a set of resources. The ServiceMonitor, PodMonitor, PrometheusRule are used. Inside the Strimzi github repository , we can get a prometheus.yml file to deploy prometheus server. This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance. We have defined our own configuration in this file . For your own deployment you have to change the target namespace, and the rules You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the Mirror Maker 2 Cluster. To be able to monitor your own on-premise Kafka cluster you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment with Prometheus setting can be found in Strimzi examples . The declarations are under the metrics stanza and define the rules to expose Kafka core features.","title":"Installation and configuration"},{"location":"monitoring/#install-prometheus","text":"After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator by first downloading the different configuration yaml files and update the namespace declaration to reflect your project name (e.g jb-kafka-strimzi ): curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-deployment.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-operator-deployment.yaml curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role.yaml > prometheus-operator-cluster-role.yaml curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role-binding.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-operator-cluster-role-binding.yaml curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-service-account.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-operator-service-account.yaml Note The prometheus-operator-deployment.yaml defines security context the operator pod will use. It is set as a non root user (unprivileged). If you need to change that, or reference an existing user modify this file. Deploy the prometheus operator, cluster role, role binding and service account (see our files under monitoring folder): oc apply -f prometheus-operator-deployment.yaml oc apply -f prometheus-operator-cluster-role.yaml oc apply -f prometheus-operator-cluster-role-binding.yaml oc apply -f prometheus-operator-service-account.yaml When you apply those configurations, the following resources are managed by the Prometheus Operator: Resource Description ClusterRole To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To manage the configuration of the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource)","title":"Install Prometheus"},{"location":"monitoring/#deploy-prometheus","text":"Note The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker or Kafka Connect monitoring, the configuration will have less rules, and parameters. See next section . Deploy the prometheus server by first changing the namespace and also by adapting the original examples/metrics/prometheus-install/prometheus.yaml file . curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: jb-kafka-strimzi/\" > prometheus.yml If you are using AlertManager (see section below ) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-rules.yaml oc apply -f prometheus-rules.yaml oc apply -f prometheus.yaml The Prometheus server configuration uses service discovery to discover the pods (Mirror Maker 2.0 pod) in the cluster from which it gets metrics.","title":"Deploy prometheus"},{"location":"monitoring/#mirror-maker-20-monitoring","text":"To monitor MM2 with Prometheus we need to add JMX Exporter and run it as Java agent.The jar file for JMX exporter agent can be found here . We copied a version in the folder mirror-maker-2/libs . We have adopted a custom mirror maker 2.0 docker imaged based on Kafka 2.4. We are detailing how to build this image using this Dockerfile in this separate note . We have used this approach as we have found an issue with the Strimzi Mirror Maker operator, that blocks us to continue the monitoring. We expect that htis operator, when it sees metrics declaration in the Mirror Maker 2 configuration yaml file, with use the JMX exporter jar. Once the Mirror Maker 2.0 is connected...","title":"Mirror maker 2.0 monitoring"},{"location":"monitoring/#install-grafana","text":"Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker. Deploy Grafan to OpenShift and expose it via a service: oc apply -f grafana.yaml In case you want to test grafana locally run: docker run -d -p 3000:3000 grafana/grafana","title":"Install Grafana"},{"location":"monitoring/#kafka-explorer","text":"","title":"Kafka Explorer"},{"location":"monitoring/#configure-grafana-dashboard","text":"To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service. Use port forwarding: export PODNAME = $( oc get pods -l name = grafana | grep grafana | awk '{print $1}' ) kubectl port-forward $PODNAME 3000 :3000 Point your browser to http://localhost:3000 . Expose the route via cli Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090","title":"Configure Grafana dashboard"},{"location":"monitoring/#alert-manager","text":"As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file The prometheus-rules.yaml . Those rules are used by the AlertManager component. Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager. Download an example of alert manager configuration file curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml > alert-manager.yaml Define a configuration for the channel to use, by starting from the following template curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml > alert-manager-config.yaml Modify this file to reflect the remote access credential and URL to the channel server. Then deploy the secret that matches your config file . oc create secret generic alertmanager-alertmanager --from-file = alertmanager.yaml = alert-manager-config.yaml oc create secret generic additional-scrape-configs --from-file = ./local-cluster/prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f -","title":"Alert Manager"},{"location":"provisioning/","text":"Provisioning Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. The base of strimzi is to define a set of kubernetes operators and custom resource definitions for the different elements of Kafka. We recommend to go over the product overview page . The service account and role binding do not need to be re-installed if you did it previously. Concept summary The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics Example of topic can be seen in section below . Kafka User are not saved part of kafka cluster but they are managed in kubernetes. For example the user credentials are saved as secret. CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster, and are provided with Strimzi for each Kafka component used in a deployment. Deployment The deployment is done in two phases: Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resource to deploy. Deploy one to many instance of those CRDs In CR yaml file the kind attribute specifies the CRD to conform to. Each CRD has a common configuration like bootstrap servers, CPU resources, logging, healthchecks... The next steps are defining how to deploy a Kafka Cluster. Create a namespace or openshift project kubectl create namespace jb-kafka-strimzi oc create project jb-kafka-strimzi Download the strimzi artefacts For the last release see this github page . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml Deploy the Custom Resource Definitions for kafka Custom resource definitions are defined within the kubernetes cluster. The following command oc apply -f install/cluster-operator/ This should create the following resource definitions: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definition oc get customresourcedefinition Deploy instances Deploy Kafka cluster The CRD for kafka cluster resource is here . Change the name of the cluster in one the yaml in the examples/kafka folder or use the strimzi/kafka-cluster.yml file in this project. For productionm we need to use persistence for the kafka log, ingress or load balancer external listener and rack awareness policies. Using non presistence: oc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 # Or kubectl apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi When looking at the pods running we can see the three kafka and zookeeper nodes as pods, and the entity operator pod. $ oc get pods my-cluster-entity-operator-645fdbc4cb-m29nk 3 /3 Running 0 18d my-cluster-kafka-0 2 /2 Running 0 3d my-cluster-kafka-1 2 /2 Running 0 3d my-cluster-kafka-2 2 /2 Running 0 3d my-cluster-zookeeper-0 2 /2 Running 0 3d my-cluster-zookeeper-1 2 /2 Running 0 3d my-cluster-zookeeper-2 2 /2 Running 0 3d strimzi-cluster-operator-58cbbcb7d-bcqhm 1 /1 Running 2 18d strimzi-topic-operator-564654cb86-nbt58 1 /1 Running 1 18d Using persistence: oc apply -f strimzi/kafka-cluster.yaml Topic Operator The role of the Topic Operator is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics. oc apply -f install/topic-operator/ -n jb-kafka-strimzi This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition Create a topic Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : my-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml -n jb-kafka-strimzi oc get kafkatopics This creates a topic test in your kafka cluster. Test with producer and consumer pods Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts locally but remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka yaml file include the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt The alias is used to access keystore entries (key and trusted certificate entries). Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol=SSL --producer-property ssl.truststore.password=password --producer-property ssl.truststore.location=/home/truststore.jks --topic test Those properties can be in file bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning Deploying Kafka Connect cluster Adding a Kafka Connect cluster on top of an existing Kafka cluster using Strimzi operators is simple. Deploying Mirror Maker 2.0 In this section we address another approach to, deploy a Kafka Connect cluster with Mirror Maker 2.0 connectors but without any local Kafka Cluster. The appraoch will be to use Event Streams on Cloud as backend Kafka cluster but use Mirror Maker for replication. The steps could","title":"Connector Provisioning"},{"location":"provisioning/#provisioning","text":"Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. The base of strimzi is to define a set of kubernetes operators and custom resource definitions for the different elements of Kafka. We recommend to go over the product overview page . The service account and role binding do not need to be re-installed if you did it previously.","title":"Provisioning"},{"location":"provisioning/#concept-summary","text":"The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics Example of topic can be seen in section below . Kafka User are not saved part of kafka cluster but they are managed in kubernetes. For example the user credentials are saved as secret. CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster, and are provided with Strimzi for each Kafka component used in a deployment.","title":"Concept summary"},{"location":"provisioning/#deployment","text":"The deployment is done in two phases: Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resource to deploy. Deploy one to many instance of those CRDs In CR yaml file the kind attribute specifies the CRD to conform to. Each CRD has a common configuration like bootstrap servers, CPU resources, logging, healthchecks... The next steps are defining how to deploy a Kafka Cluster.","title":"Deployment"},{"location":"provisioning/#create-a-namespace-or-openshift-project","text":"kubectl create namespace jb-kafka-strimzi oc create project jb-kafka-strimzi","title":"Create a namespace or openshift project"},{"location":"provisioning/#download-the-strimzi-artefacts","text":"For the last release see this github page . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml","title":"Download the strimzi artefacts"},{"location":"provisioning/#deploy-the-custom-resource-definitions-for-kafka","text":"Custom resource definitions are defined within the kubernetes cluster. The following command oc apply -f install/cluster-operator/ This should create the following resource definitions: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definition oc get customresourcedefinition","title":"Deploy the Custom Resource Definitions for kafka"},{"location":"provisioning/#deploy-instances","text":"","title":"Deploy instances"},{"location":"provisioning/#deploy-kafka-cluster","text":"The CRD for kafka cluster resource is here . Change the name of the cluster in one the yaml in the examples/kafka folder or use the strimzi/kafka-cluster.yml file in this project. For productionm we need to use persistence for the kafka log, ingress or load balancer external listener and rack awareness policies. Using non presistence: oc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 # Or kubectl apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi When looking at the pods running we can see the three kafka and zookeeper nodes as pods, and the entity operator pod. $ oc get pods my-cluster-entity-operator-645fdbc4cb-m29nk 3 /3 Running 0 18d my-cluster-kafka-0 2 /2 Running 0 3d my-cluster-kafka-1 2 /2 Running 0 3d my-cluster-kafka-2 2 /2 Running 0 3d my-cluster-zookeeper-0 2 /2 Running 0 3d my-cluster-zookeeper-1 2 /2 Running 0 3d my-cluster-zookeeper-2 2 /2 Running 0 3d strimzi-cluster-operator-58cbbcb7d-bcqhm 1 /1 Running 2 18d strimzi-topic-operator-564654cb86-nbt58 1 /1 Running 1 18d Using persistence: oc apply -f strimzi/kafka-cluster.yaml","title":"Deploy Kafka cluster"},{"location":"provisioning/#topic-operator","text":"The role of the Topic Operator is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics. oc apply -f install/topic-operator/ -n jb-kafka-strimzi This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition","title":"Topic Operator"},{"location":"provisioning/#create-a-topic","text":"Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : my-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml -n jb-kafka-strimzi oc get kafkatopics This creates a topic test in your kafka cluster.","title":"Create a topic"},{"location":"provisioning/#test-with-producer-and-consumer-pods","text":"Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts locally but remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka yaml file include the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt The alias is used to access keystore entries (key and trusted certificate entries). Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol=SSL --producer-property ssl.truststore.password=password --producer-property ssl.truststore.location=/home/truststore.jks --topic test Those properties can be in file bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Test with producer and consumer pods"},{"location":"provisioning/#deploying-kafka-connect-cluster","text":"Adding a Kafka Connect cluster on top of an existing Kafka cluster using Strimzi operators is simple.","title":"Deploying Kafka Connect cluster"},{"location":"provisioning/#deploying-mirror-maker-20","text":"In this section we address another approach to, deploy a Kafka Connect cluster with Mirror Maker 2.0 connectors but without any local Kafka Cluster. The appraoch will be to use Event Streams on Cloud as backend Kafka cluster but use Mirror Maker for replication. The steps could","title":"Deploying Mirror Maker 2.0"},{"location":"sc2-mm2/","text":"Running a custom Mirror Maker 2.0 In this approach we are using properties file to define the Mirror Maker 2.0 configuration, package JMX exporter with it inside a docker image and deploy the image to Openshift. The configuration approach supports the replication from local on-premise cluster running on kubernetes cluster to Event Streams on the Cloud. Define the MM configuration The configuration define the source and target cluster and the security settings for both clusters. As the goal is to run within the same OpenShift cluster as Kafka, the broker list for the source matches the URL within the broker service: # get the service URL oc describe svc my-cluster-kafka-bootstrap # URL my-cluster-kafka-bootstrap:9092 The target cluster uses the bootstrap servers from the Event Streams Credentials, and the API KEY is defined with the manager role, so mirror maker can create topic dynamically. Properties template file can be seen here: kafka-to-es-mm2 clusters = source, target source.bootstrap.servers = my-cluster-kafka-bootstrap:9092 source.ssl.endpoint.identification.algorithm = target.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username = \"token\" password=\"<Manager API KEY from Event Streams>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Upload the properties as a secret oc create secret generic mm2-std-properties --from-file = local-cluster/es-to-localkafka-mm2.properties Defining a custom docker image Starting from the Kafka 2.4 image, we need to add Prometheus JMX exporter and use the properties file as argument. The docker file is here . FROM strimzi/kafka:latest-kafka-2.4.0 # ... ARG PROP_NAME = local-cluster/es-to-localkafka-mm2.properties ENV LOG_DIR = /tmp/logs ENV EXTRA_ARGS = \"-javaagent:/usr/local/share/jars/jmx_prometheus_javaagent-0.12.0.jar=9400:/etc/jmx_exporter/jmx_exporter.yaml \" COPY $PROP_NAME /home/mm2.properties # .... EXPOSE 9400 CMD /opt/kafka/bin/connect-mirror-maker.sh /home/mm2.properties The file could be copied inside the docker image or better mounted from secret when deployed to kubernetes. Build and push the image to a docker registry. docker build -t ibmcase/mm2ocp:v0.0.2 --build-arg = local-cluster/kafka-to-es-mm2.properties . docker push ibmcase/mm2ocp:v0.0.2 From Event Streams on Cloud to Local Cloud The approach is similar to the above steps except we use another properties file: The properties to use is es-to-kafka-mm2.properties clusters = source, target target.bootstrap.servers = my-cluster-kafka-bootstrap:9092 target.ssl.endpoint.identification.algorithm = source.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.event.... The image to build is using this properties file: docker build -t ibmcase/mm2ocp:v0.0.3 --build-arg = es-cluster/es-to-kafka-mm2.properties . Once deployed to Openshift, use the producer and consumer to test the mirroring. Define the monitoring rules As explained in the monitoring note , we need to define the Prometheus rules within a yaml file : lowercaseOutputName : true lowercaseOutputLabelNames : true rules : - pattern : \"kafka.connect<type=connect-worker-metrics>([^:]+):\" name : \"kafka_connect_connect_worker_metrics_$1\" - pattern : \"kafka.connect<type=connect-metrics, client-id=([^:]+)><>([^:]+)\" name : \"kafka_connect_connect_metrics_$1_$2\" # Rules below match the Kafka Connect/MirrorMaker MBeans in the jconsole order # Worker task states - pattern : kafka.connect<type=connect-worker-metrics, connector=(\\w+)><>(connector-destroyed-task-count|connector-failed-task-count|connector-paused-task-count|connector-running-task-count|connector-total-task-count|connector-unassigned-task-count) name : connect_worker_metrics_$1_$2 # Task metrics - pattern : kafka.connect<type=connector-task-metrics, connector=(\\w+), task=(\\d+)><>(batch-size-avg|batch-size-max|offset-commit-avg-time-ms|offset-commit-failure-percentage|offset-commit-max-time-ms|offset-commit-success-percentage|running-ratio) name : connect_connector_task_metrics_$1_$3 labels : task : \"$2\" # Source task metrics - pattern : kafka.connect<type=source-task-metrics, connector=(\\w+), task=(\\d+)><>(source-record-active-count|source-record-poll-total|source-record-write-total) name : connect_source_task_metrics_$1_$3 labels : task : \"$2\" # Task errors - pattern : kafka.connect<type=task-error-metrics, connector=(\\w+), task=(\\d+)><>(total-record-errors|total-record-failures|total-records-skipped|total-retries) name : connect_task_error_metrics_$1_$3 labels : task : \"$2\" # CheckpointConnector metrics - pattern : kafka.connect.mirror<type=MirrorCheckpointConnector, source=(.+), target=(.+), group=(.+), topic=(.+), partition=(\\d+)><>(checkpoint-latency-ms) name : connect_mirror_mirrorcheckpointconnector_$6 labels : source : \"$1\" target : \"$2\" group : \"$3\" topic : \"$4\" partition : \"$5\" # SourceConnector metrics - pattern : kafka.connect.mirror<type=MirrorSourceConnector, target=(.+), topic=(.+), partition=(\\d+)><>(byte-rate|byte-count|record-age-ms|record-rate|record-count|replication-latency-ms) name : connect_mirror_mirrorsourceconnector_$4 labels : target : \"$1\" topic : \"$2\" partition : \"$3\" Then upload this properties file in a secret (the following command update an existing config as predefined by the Prometheus deployment) oc create secret generic mm2-jmx-exporter --from-file = ./mm2-jmx-exporter.yaml Deploy the application As we are using secret to mount file we want to use a deployment.yml to define the Mirror Maker deployment # Under mirror-maker-2 folder oc apply -f mm2-deployment.yaml To undeploy everything oc delete all -l app = mm2ocp","title":"Running a custom Mirror Maker 2.0"},{"location":"sc2-mm2/#running-a-custom-mirror-maker-20","text":"In this approach we are using properties file to define the Mirror Maker 2.0 configuration, package JMX exporter with it inside a docker image and deploy the image to Openshift. The configuration approach supports the replication from local on-premise cluster running on kubernetes cluster to Event Streams on the Cloud.","title":"Running a custom Mirror Maker 2.0"},{"location":"sc2-mm2/#define-the-mm-configuration","text":"The configuration define the source and target cluster and the security settings for both clusters. As the goal is to run within the same OpenShift cluster as Kafka, the broker list for the source matches the URL within the broker service: # get the service URL oc describe svc my-cluster-kafka-bootstrap # URL my-cluster-kafka-bootstrap:9092 The target cluster uses the bootstrap servers from the Event Streams Credentials, and the API KEY is defined with the manager role, so mirror maker can create topic dynamically. Properties template file can be seen here: kafka-to-es-mm2 clusters = source, target source.bootstrap.servers = my-cluster-kafka-bootstrap:9092 source.ssl.endpoint.identification.algorithm = target.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username = \"token\" password=\"<Manager API KEY from Event Streams>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Upload the properties as a secret oc create secret generic mm2-std-properties --from-file = local-cluster/es-to-localkafka-mm2.properties","title":"Define the MM configuration"},{"location":"sc2-mm2/#defining-a-custom-docker-image","text":"Starting from the Kafka 2.4 image, we need to add Prometheus JMX exporter and use the properties file as argument. The docker file is here . FROM strimzi/kafka:latest-kafka-2.4.0 # ... ARG PROP_NAME = local-cluster/es-to-localkafka-mm2.properties ENV LOG_DIR = /tmp/logs ENV EXTRA_ARGS = \"-javaagent:/usr/local/share/jars/jmx_prometheus_javaagent-0.12.0.jar=9400:/etc/jmx_exporter/jmx_exporter.yaml \" COPY $PROP_NAME /home/mm2.properties # .... EXPOSE 9400 CMD /opt/kafka/bin/connect-mirror-maker.sh /home/mm2.properties The file could be copied inside the docker image or better mounted from secret when deployed to kubernetes. Build and push the image to a docker registry. docker build -t ibmcase/mm2ocp:v0.0.2 --build-arg = local-cluster/kafka-to-es-mm2.properties . docker push ibmcase/mm2ocp:v0.0.2","title":"Defining a custom docker image"},{"location":"sc2-mm2/#from-event-streams-on-cloud-to-local-cloud","text":"The approach is similar to the above steps except we use another properties file: The properties to use is es-to-kafka-mm2.properties clusters = source, target target.bootstrap.servers = my-cluster-kafka-bootstrap:9092 target.ssl.endpoint.identification.algorithm = source.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.event.... The image to build is using this properties file: docker build -t ibmcase/mm2ocp:v0.0.3 --build-arg = es-cluster/es-to-kafka-mm2.properties . Once deployed to Openshift, use the producer and consumer to test the mirroring.","title":"From Event Streams on Cloud to Local Cloud"},{"location":"sc2-mm2/#define-the-monitoring-rules","text":"As explained in the monitoring note , we need to define the Prometheus rules within a yaml file : lowercaseOutputName : true lowercaseOutputLabelNames : true rules : - pattern : \"kafka.connect<type=connect-worker-metrics>([^:]+):\" name : \"kafka_connect_connect_worker_metrics_$1\" - pattern : \"kafka.connect<type=connect-metrics, client-id=([^:]+)><>([^:]+)\" name : \"kafka_connect_connect_metrics_$1_$2\" # Rules below match the Kafka Connect/MirrorMaker MBeans in the jconsole order # Worker task states - pattern : kafka.connect<type=connect-worker-metrics, connector=(\\w+)><>(connector-destroyed-task-count|connector-failed-task-count|connector-paused-task-count|connector-running-task-count|connector-total-task-count|connector-unassigned-task-count) name : connect_worker_metrics_$1_$2 # Task metrics - pattern : kafka.connect<type=connector-task-metrics, connector=(\\w+), task=(\\d+)><>(batch-size-avg|batch-size-max|offset-commit-avg-time-ms|offset-commit-failure-percentage|offset-commit-max-time-ms|offset-commit-success-percentage|running-ratio) name : connect_connector_task_metrics_$1_$3 labels : task : \"$2\" # Source task metrics - pattern : kafka.connect<type=source-task-metrics, connector=(\\w+), task=(\\d+)><>(source-record-active-count|source-record-poll-total|source-record-write-total) name : connect_source_task_metrics_$1_$3 labels : task : \"$2\" # Task errors - pattern : kafka.connect<type=task-error-metrics, connector=(\\w+), task=(\\d+)><>(total-record-errors|total-record-failures|total-records-skipped|total-retries) name : connect_task_error_metrics_$1_$3 labels : task : \"$2\" # CheckpointConnector metrics - pattern : kafka.connect.mirror<type=MirrorCheckpointConnector, source=(.+), target=(.+), group=(.+), topic=(.+), partition=(\\d+)><>(checkpoint-latency-ms) name : connect_mirror_mirrorcheckpointconnector_$6 labels : source : \"$1\" target : \"$2\" group : \"$3\" topic : \"$4\" partition : \"$5\" # SourceConnector metrics - pattern : kafka.connect.mirror<type=MirrorSourceConnector, target=(.+), topic=(.+), partition=(\\d+)><>(byte-rate|byte-count|record-age-ms|record-rate|record-count|replication-latency-ms) name : connect_mirror_mirrorsourceconnector_$4 labels : target : \"$1\" topic : \"$2\" partition : \"$3\" Then upload this properties file in a secret (the following command update an existing config as predefined by the Prometheus deployment) oc create secret generic mm2-jmx-exporter --from-file = ./mm2-jmx-exporter.yaml","title":"Define the monitoring rules"},{"location":"sc2-mm2/#deploy-the-application","text":"As we are using secret to mount file we want to use a deployment.yml to define the Mirror Maker deployment # Under mirror-maker-2 folder oc apply -f mm2-deployment.yaml To undeploy everything oc delete all -l app = mm2ocp","title":"Deploy the application"}]}